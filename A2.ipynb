{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values in title: 0\n",
      "Number of missing values in release_date: 24\n",
      "Number of missing values in genres: 0\n",
      "Number of missing values in overview: 78\n",
      "Number of missing values in popularity: 2\n",
      "Number of missing values in revenue: 2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import sklearn\n",
    "\n",
    "# Load the dataset\n",
    "original_df = pd.read_csv('top_10000_popular_movies_tmdb.csv')\n",
    "\n",
    "# Select the relevant columns\n",
    "df = original_df[['title', 'release_date', 'genres', 'overview', 'popularity', 'revenue']]\n",
    "\n",
    "# Create a new row with null values\n",
    "null_row = pd.DataFrame([{\n",
    "    'title': 'Null movie',\n",
    "    'release_date': float('nan'),\n",
    "    'genres': \"[]\",\n",
    "    'overview': float('nan'),\n",
    "    'popularity': float('nan'),\n",
    "    'revenue': 0\n",
    "}])\n",
    "\n",
    "df = pd.concat([null_row, df], ignore_index=True)\n",
    "\n",
    "# Finds missing values in the dataset\n",
    "for feature in df.columns:\n",
    "    missing_data = df[df[feature].isnull()]\n",
    "    print(f\"Number of missing values in {feature}: {len(missing_data)}\")\n",
    "\n",
    "# Convert each row to a formatted string and store in a new column\n",
    "df['formatted_string'] = df.apply(\n",
    "    lambda row: f\"{row['title']}, released on {'an unknown date' if pd.isnull(row['release_date']) else row['release_date']}, is a {'movie with unknown genre(s)' if row['genres'] == '[]' else f'{row['genres']} movie'} with a plot of: {'movie\\'s plot is unknown.' if pd.isnull(row['overview']) else f'that is about {row['overview']}'} It has a popularity score of {'an unknown amount' if pd.isnull(row['popularity']) else row['popularity']}, assigned to the movie by TMDB based on user engagement. It generated {'an unknown amount' if row['revenue'] == 0 else f'{row['revenue']} USD'} in revenue.\",\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model_BAAI = SentenceTransformer('BAAI/bge-small-en')\n",
    "\n",
    "# Encode the formatted strings\n",
    "embeddings = pd.Series(df[\"formatted_string\"]).apply(lambda x: model_BAAI.encode(str(x)))\n",
    "\n",
    "# Add the embeddings to the DataFrame\n",
    "df[\"embedding\"] = embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 2 formatted strings with the highest cosine similarities:\n",
      "Movie: The Super Mario Bros. Movie, released on 2023-04-05, is a ['Animation', 'Family', 'Adventure', 'Fantasy', 'Comedy'] movie with a plot of: that is about While working underground to fix a water main, Brooklyn plumbers—and brothers—Mario and Luigi are transported down a mysterious pipe and wander into a magical new world. But when the brothers are separated, Mario embarks on an epic quest to find Luigi. It has a popularity score of 3394.458, assigned to the movie by TMDB based on user engagement. It generated 1308766975.0 USD in revenue.\n",
      "Cosine similarity: 0.870617151260376\n",
      "\n",
      "Movie: Super Mario Bros., released on 1993-05-28, is a ['Adventure', 'Fantasy', 'Comedy', 'Family', 'Science Fiction'] movie with a plot of: that is about Mario and Luigi, plumbers from Brooklyn, find themselves in an alternate universe where evolved dinosaurs live in hi-tech squalor. They're the only hope to save our universe from invasion by the dino dictator, Koopa. It has a popularity score of 35.011, assigned to the movie by TMDB based on user engagement. It generated 20915465.0 USD in revenue.\n",
      "Cosine similarity: 0.8682831525802612\n",
      "\n",
      "Give me the plot of the super mario bros movie.:\n",
      "['Mario and Luigi are transported down a mysterious pipe and wander into a magical new world.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model_flan = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "# Define the query\n",
    "query = \"Give me the plot of the super mario bros movie.\"\n",
    "#query = \"Which movie was released first? the Dark Knight or Crater\"\n",
    "#query = \"how much revenue did the movie the dark knight make?\"\n",
    "\n",
    "query_embedding = model_BAAI.encode(query)\n",
    "\n",
    "# Compute cosine similarity for each formatted string\n",
    "similarity_dict = {}\n",
    "for index, row in df.iterrows():\n",
    "    cosine_sim = sklearn.metrics.pairwise.cosine_similarity([row[\"embedding\"]], [query_embedding])[0][0]\n",
    "    similarity_dict[row[\"formatted_string\"]] = cosine_sim\n",
    "\n",
    "# Sort the similarities in descending order and get the top k formatted strings\n",
    "k = 2\n",
    "top_k_strings = sorted(similarity_dict.items(), key=lambda item: item[1], reverse=True)[:k]\n",
    "\n",
    "# Print the top k formatted strings with their cosine similarities\n",
    "print(f\"Top {k} formatted strings with the highest cosine similarities:\")\n",
    "for formatted_string, similarity in top_k_strings:\n",
    "    print(f\"Movie: {formatted_string}\")\n",
    "    print(f\"Cosine similarity: {similarity}\\n\")\n",
    "\n",
    "# Use the formatted string with the highest cosine similarity in the instruction prompt\n",
    "instruction_prompt = f\"Based on the following information, {query}: {top_k_strings} don't make up any new infotmation just use the information given.\"\n",
    "\n",
    "inputs = tokenizer(instruction_prompt, return_tensors=\"pt\")\n",
    "outputs = model_flan.generate(**inputs)\n",
    "print(f\"{query}:\")\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
