{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2_MCDA_5511\n",
    "Assignment 2 for MCDA 5511\n",
    "\n",
    "To get the code to run you must\n",
    "\n",
    "1. Run \"uv sync\"\n",
    "2. Select the python \"3.10.16\" kernal\n",
    "3. Run .venv\\Scripts\\Activate\n",
    "4. Run .venv\\Scripts\\python.exe -m ensurepip --upgrade\n",
    "5. Run .venv\\Scripts\\python.exe -m pip install ipykernel\n",
    "\n",
    "If you are on Mac you may need to switch every '\\' to a '/' in the above commands.\n",
    "\n",
    "## Question 1\n",
    "\n",
    "### Sampling / Processing\n",
    "\n",
    "We had originally looked into sampling our data to make it more manageable, however upon futher research we found that the downsides to doing so did not outweigh the benefits. Too much bias would be introduced into the training of the model, and the dataset was not big enough that the performance would increase substantially.\n",
    "\n",
    "For an example of bias, If we used something like stratified sampling, the only way we found to divide up the data would be by looking at the movie genres. This causes issue because some movies fall into multiple genres, so looking at every possible combination would yeild 9033 unique classifications. Since groups must be mutually exclusive, stratified sampling would be of no use to us.\n",
    "\n",
    "There could be some merit in clustered sampling, however we decided that our dataset would be small enough that sampling was not needed.\n",
    "\n",
    "We instead chose to cut out some non relevent features to cut down on processing time instead. The features we deemed relevant for this model were the title, release date, genre(s), overview, popularity and revenue.\n",
    "\n",
    "### Statistics\n",
    "We ran some calculations to find some basic statistics of our data set. Here is a graph showing the spread of document length:\n",
    "\n",
    "![document_length_graph](document_length.png)\n",
    "\n",
    "We found that the average length of an overview was 272.23 characters. This only involved the data for the entry in the data set, the actual data passed to the model was longer since we processed all the features into one continuous string. In reality this equated to about 75 words per document on average after data processing.\n",
    "\n",
    "As for vocabulary, we ran 2 tests. One to find the most common words, and one to find the most common words (excluding stopwords):\n",
    "\n",
    "![top_10_words](top_10.png)\n",
    "\n",
    "![top_10_no_stopwords](top_10_no_stopwords.png)\n",
    "\n",
    "### Topics\n",
    "\n",
    "The main topics covered in this dataset include statistics surrounding the top 10000 movies on TMDB. This includes topics such as its title, revenue, an overview, popularity, etc. Its important to note that not all movies have data for each feature, but every movie at least has a title. We found that 78 of the movies did not have overviews, 24 did not have a release date, 2 did not have a popularity rating, and 2 did not have a revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values in title: 0\n",
      "Number of missing values in release_date: 24\n",
      "Number of missing values in genres: 0\n",
      "Number of missing values in overview: 78\n",
      "Number of missing values in popularity: 2\n",
      "Number of missing values in revenue: 2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import sklearn\n",
    "\n",
    "# Load the dataset\n",
    "original_df = pd.read_csv('top_10000_popular_movies_tmdb.csv')\n",
    "\n",
    "# Select the relevant columns\n",
    "df = original_df[['title', 'release_date', 'genres', 'overview', 'popularity', 'revenue']]\n",
    "\n",
    "# Create a new row with null values\n",
    "null_row = pd.DataFrame([{\n",
    "    'title': 'Null movie',\n",
    "    'release_date': float('nan'),\n",
    "    'genres': \"[]\",\n",
    "    'overview': float('nan'),\n",
    "    'popularity': float('nan'),\n",
    "    'revenue': 0\n",
    "}])\n",
    "\n",
    "df = pd.concat([null_row, df], ignore_index=True)\n",
    "\n",
    "# Finds missing values in the dataset\n",
    "for feature in df.columns:\n",
    "    missing_data = df[df[feature].isnull()]\n",
    "    print(f\"Number of missing values in {feature}: {len(missing_data)}\")\n",
    "\n",
    "# Convert each row to a formatted string and store in a new column\n",
    "df['formatted_string'] = df.apply(\n",
    "    lambda row: f\"{row['title']}, released on {'an unknown date' if pd.isnull(row['release_date']) else row['release_date']}, is a {'movie with unknown genre(s)' if row['genres'] == '[]' else f'{row['genres']} movie'} with a plot of: {'movie\\'s plot is unknown.' if pd.isnull(row['overview']) else f'that is about {row['overview']}'} It has a popularity score of {'an unknown amount' if pd.isnull(row['popularity']) else row['popularity']}, assigned to the movie by TMDB based on user engagement. It generated {'an unknown amount' if row['revenue'] == 0 else f'{row['revenue']} USD'} in revenue.\",\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model_BAAI = SentenceTransformer('BAAI/bge-small-en')\n",
    "\n",
    "# Encode the formatted strings\n",
    "embeddings = pd.Series(df[\"formatted_string\"]).apply(lambda x: model_BAAI.encode(str(x)))\n",
    "\n",
    "# Add the embeddings to the DataFrame\n",
    "df[\"embedding\"] = embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 2 formatted strings with the highest cosine similarities:\n",
      "Movie: The Super Mario Bros. Movie, released on 2023-04-05, is a ['Animation', 'Family', 'Adventure', 'Fantasy', 'Comedy'] movie with a plot of: that is about While working underground to fix a water main, Brooklyn plumbers—and brothers—Mario and Luigi are transported down a mysterious pipe and wander into a magical new world. But when the brothers are separated, Mario embarks on an epic quest to find Luigi. It has a popularity score of 3394.458, assigned to the movie by TMDB based on user engagement. It generated 1308766975.0 USD in revenue.\n",
      "Cosine similarity: 0.870617151260376\n",
      "\n",
      "Movie: Super Mario Bros., released on 1993-05-28, is a ['Adventure', 'Fantasy', 'Comedy', 'Family', 'Science Fiction'] movie with a plot of: that is about Mario and Luigi, plumbers from Brooklyn, find themselves in an alternate universe where evolved dinosaurs live in hi-tech squalor. They're the only hope to save our universe from invasion by the dino dictator, Koopa. It has a popularity score of 35.011, assigned to the movie by TMDB based on user engagement. It generated 20915465.0 USD in revenue.\n",
      "Cosine similarity: 0.8682831525802612\n",
      "\n",
      "Give me the plot of the super mario bros movie.:\n",
      "['Mario and Luigi are transported down a mysterious pipe and wander into a magical new world.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model_flan = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "# Define the query\n",
    "query = \"Give me the plot of the super mario bros movie.\"\n",
    "#query = \"Which movie was released first? the Dark Knight or Crater\"\n",
    "#query = \"how much revenue did the movie the dark knight make?\"\n",
    "\n",
    "query_embedding = model_BAAI.encode(query)\n",
    "\n",
    "# Compute cosine similarity for each formatted string\n",
    "similarity_dict = {}\n",
    "for index, row in df.iterrows():\n",
    "    cosine_sim = sklearn.metrics.pairwise.cosine_similarity([row[\"embedding\"]], [query_embedding])[0][0]\n",
    "    similarity_dict[row[\"formatted_string\"]] = cosine_sim\n",
    "\n",
    "# Sort the similarities in descending order and get the top k formatted strings\n",
    "k = 2\n",
    "top_k_strings = sorted(similarity_dict.items(), key=lambda item: item[1], reverse=True)[:k]\n",
    "\n",
    "# Print the top k formatted strings with their cosine similarities\n",
    "print(f\"Top {k} formatted strings with the highest cosine similarities:\")\n",
    "for formatted_string, similarity in top_k_strings:\n",
    "    print(f\"Movie: {formatted_string}\")\n",
    "    print(f\"Cosine similarity: {similarity}\\n\")\n",
    "\n",
    "# Use the formatted string with the highest cosine similarity in the instruction prompt\n",
    "instruction_prompt = f\"Based on the following information, {query}: {top_k_strings} don't make up any new infotmation just use the information given.\"\n",
    "\n",
    "inputs = tokenizer(instruction_prompt, return_tensors=\"pt\")\n",
    "outputs = model_flan.generate(**inputs)\n",
    "print(f\"{query}:\")\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
